# Phase 2A: Document Ingestion Pipeline

## Brief Description

Implement the document processing and ingestion pipeline using the open-source Docling library that extracts text and structure from uploaded files (PDF, DOCX, TXT, HTML, MD), applies semantic chunking using Docling's HybridChunker, generates embeddings using OpenAI API, and stores the vectors in Supabase. This phase leverages Docling's advanced document understanding capabilities for better content extraction and chunking.

## Files to Create:
- `src/ingest.py` - Document processing and ingestion pipeline using Docling
- `src/docling_converter.py` - Docling document converter wrapper
- `src/docling_chunker.py` - Docling HybridChunker integration with custom configuration
- `src/embeddings.py` - OpenAI embedding generation
- `tests/test_ingest.py` - Unit tests for ingestion pipeline
- `tests/test_docling_converter.py` - Unit tests for Docling conversion
- `tests/test_docling_chunker.py` - Unit tests for Docling chunking
- `tests/test_embeddings.py` - Unit tests for embedding generation

## Ingestion Algorithm:
1. **Document Conversion**: Use Docling's DocumentConverter to parse and extract structured content:
   - PDF: Advanced layout understanding with table/figure detection
   - DOCX: Native document structure preservation
   - TXT/MD: Direct text processing with structure detection
   - HTML: Clean text extraction with semantic structure
   - PNG/JPG: OCR capabilities for image documents
2. **Semantic Chunking**: Apply Docling's HybridChunker with HuggingFace tokenizer for intelligent content-aware chunking
3. **Embedding Generation**: Call OpenAI text-embedding-3-small API for each chunk
4. **Database Storage**: Insert document metadata and vector chunks into Supabase with enhanced metadata from Docling
5. **Error Handling**: Validate file types, handle conversion failures, API errors, database errors

## Key Functions:
- `convert_document_with_docling(file_path: str) -> DoclingDocument`
- `setup_docling_chunker(max_tokens: int = 512) -> HybridChunker`
- `chunk_docling_document(doc: DoclingDocument, chunker: HybridChunker) -> List[DoclingChunk]`
- `generate_embeddings(texts: List[str]) -> List[List[float]]`
- `store_document_and_vectors(filename: str, chunks: List[DoclingChunk], embeddings: List[List[float]]) -> str`

## Docling Integration Strategy:
- **DocumentConverter**: Unified parsing for all supported file formats with advanced PDF understanding
- **HybridChunker**: Content-aware chunking that respects document structure (headings, paragraphs, tables, lists)
- **Tokenizer Integration**: Use HuggingFace tokenizer matching embedding model (sentence-transformers/all-MiniLM-L6-v2)
- **Metadata Preservation**: Maintain document structure information (page numbers, section headers, table content)
- **Table and Figure Handling**: Special processing for structured content with appropriate serialization

## Dependencies Required:
- `docling` - Core Docling library for document conversion
- `docling-core[chunking]` - Chunking functionality with tokenizer support
- `transformers` - HuggingFace tokenizers for chunking
- `openai` - OpenAI API client for embeddings
- `supabase` - Database client for vector storage
- `numpy<2.0` - NumPy compatibility for macOS support

## Docling Configuration Details:
- **DocumentConverter Setup**: Initialize with default pipeline for comprehensive document understanding
- **HybridChunker Configuration**: 
  - Use HuggingFaceTokenizer with sentence-transformers/all-MiniLM-L6-v2 model
  - Set max_tokens to 512 for optimal embedding performance
  - Enable content-aware chunking that respects document structure
- **Pipeline Options**: Configure for table extraction, figure detection, and metadata preservation
- **Serialization**: Use MarkdownParams for clean text representation with image placeholders

## Error Handling Requirements:
- Validate file types and size limits before Docling processing
- Handle Docling conversion errors and unsupported document formats gracefully
- Implement retry logic for OpenAI API calls with exponential backoff
- Rollback database operations on failure
- Log detailed errors including Docling conversion status and chunk metadata

## Testing Requirements:
- Unit tests for Docling document conversion with various file formats
- Test HybridChunker configuration and chunking quality
- Mock OpenAI API calls for consistent embedding testing
- Test database storage with Docling metadata preservation
- Integration tests for complete Docling-based ingestion workflow
- Performance tests comparing Docling vs traditional parsing methods
