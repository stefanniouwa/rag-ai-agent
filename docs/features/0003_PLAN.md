# Phase 2B: Query and Retrieval System

## Brief Description

Implement the query processing and retrieval system that embeds user questions, performs vector similarity search in Supabase, orchestrates LLM response generation with retrieved context, and manages chat memory for multi-turn conversations. This phase enables users to query their document corpus and receive contextual answers with citations.

## Files to Create:
- `src/query.py` - Vector search and retrieval
- `src/chat.py` - LLM orchestration and response formatting
- `src/memory.py` - Chat memory management (5-turn context window)
- `tests/test_query.py` - Unit tests for vector search
- `tests/test_chat.py` - Unit tests for chat orchestration
- `tests/test_memory.py` - Unit tests for memory management

## Query Algorithm:
1. **Query Embedding**: Generate embedding for user question using OpenAI API
2. **Vector Search**: Perform cosine similarity search in Supabase pgvector (top-k=4)
3. **Context Preparation**: Combine retrieved chunks with chat history (last 5 turns)
4. **LLM Generation**: Send context to OpenAI GPT-4.1-mini with citation instructions
5. **Response Formatting**: Parse response and format citations with source metadata
6. **Memory Update**: Store user question and AI response in chat_histories table

## Key Functions:
- `embed_query(query: str) -> List[float]`
- `vector_search(query_embedding: List[float], top_k: int = 4) -> List[VectorChunk]`
- `get_chat_memory(session_id: str, limit: int = 5) -> List[ChatMessage]`
- `generate_response(query: str, context_chunks: List[VectorChunk], chat_history: List[ChatMessage]) -> str`
- `store_chat_turn(session_id: str, user_message: str, ai_response: str) -> None`

## Vector Search Implementation:
- Use cosine similarity for semantic matching
- Index optimization with ivfflat for faster approximate search
- Top-k retrieval (k=4) to balance relevance and context window limits
- Metadata filtering capability for future enhancements

## Chat Memory Management:
- Rolling window of last 5 conversation turns
- Store both user questions and AI responses
- Include memory in LLM context for coherent multi-turn conversations
- Session isolation using user-based session IDs

## Citation System:
- Include source document filename and chunk metadata in LLM prompt
- Parse structured citations from LLM response
- Display citations with snippet previews and source links
- Track citation accuracy for quality monitoring

## LLM Prompt Engineering:
- System prompt instructing the model to provide citations
- Context window management to stay within token limits
- Structured response format for easy citation parsing
- Fallback handling when no relevant documents are found

## Dependencies Required:
- `openai` - OpenAI API client for embeddings and chat completion
- `supabase` - Vector database operations
- `numpy` - Vector similarity calculations (if needed)

## Performance Considerations:
- Cache embeddings for repeated queries
- Optimize vector search with proper indexing
- Implement query result caching for identical questions
- Monitor API usage and implement rate limiting

## Testing Requirements:
- Unit tests for vector search functionality
- Mock OpenAI API calls for consistent testing
- Test chat memory storage and retrieval
- Test citation parsing and formatting
- Integration tests for complete query workflow
- Performance tests for search latency
